{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for REU 2019\n",
    "\n",
    "_Burt Rosenberg, 21 May 2019_\n",
    "\n",
    "\n",
    "### Supplement: Bayes\n",
    "\n",
    "This draws heavily from Thinking Bayes by Allen Downey.\n",
    "\n",
    "We have already done some computation Bayesian Statistics, in the M &amp; M's problem. Recall the idea:\n",
    "\n",
    "> In Baysian statistics, probably is analogous to belief. The higher the probablity the more one believes that the event is the case. It is the probability of the Weather Channel. Tomorrow will occur and it will occur only once &mdash; to say _\"50 out of 100 times it was March 22nd, 2019,  it rained\"_ makes no sense.\n",
    "\n",
    "The mantra of Bayesnian statistics is that the subjectivity is captured in the _prior_. But once that subjectivity is accounted for, the rest is objective. Data will update that prior by providing supporting or refuting evidence, and the posterior probability reflects the rational person's belief, given the prior and the evidence.\n",
    "\n",
    "The formula is, given <code>P(H<sub>i</sub>)</code>, the prior of the probability of <code>H<sub>i</sub></code>, and the occurence <code>D</code>, what is the new belief that <code>H<sub>i</sub></code> is the case? I.e. <code>P(H<sub>i</sub>|D)</code>? This is derived as \n",
    "\n",
    "> <code>P(H<sub>i</sub>|D) P(D) = P(D|H<sub>i</sub>) P(H<sub>i</sub>)</code>\n",
    "\n",
    "the symbol <code>P(D|H<sub>i</sub>)</code> is called the _likelihood_. One avoids calculating the <code>P(D)</code> normalizing factor by normalizing instead normalizing the collection <code>{P(H<sub>i</sub>|D) P(H<sub>i</sub>)}<sub>i</sub></code>, given that the <code>H<sub>i</sub></code> are mutually exclusive and collectively exhaustive.\n",
    "\n",
    "\n",
    "__The Train Problem__\n",
    "\n",
    "We want to estimate the number of trains owned by a certain railroad company, given the observation of trains, and the train number. We know that the company numbers its trains consecutively starting with one. If we observe train 60, for instance, then we know that company owns at least 60 trains. We make several measurements and then provide a guess as to how many trains we haven't seen.\n",
    "\n",
    "This problem considers two priors. One prior is a uniform distribution over a range from 1 to n. That is, we assume there are some trains, but never more than n, and we have no preference for one number of trains compared to another. This results in some answer. However, things being what they are, it seems silly to think that there is a magic n, and all train companies choose a number uniformly between 1 and n, and that's the number of trains they run. Rather we suspect that the number of trains a company owns follows a power law. And using a power law distribution we find that our estimations are less sensitive to the arbitrary parameter choices we make when choosing a prior.\n",
    "\n",
    "That's a good thing.\n",
    "\n",
    "The code demonstrates some features of classes. There is a base class for a Probability Mass Function, and particular PMF's inherit from it. They enhance the class by initilizing a discretized PMF according to either the Uniform or the Power Law distribution.\n",
    "\n",
    "The Bayes class contains the basic framework for estimation. On creation it is provided a prior, in the form of an initialized PMF object. The exact type of the PMF object will be a subclass, depending on distribution, but all share the class PMF which, and this is the functionality needed for the Bayes class. \n",
    "\n",
    "The Bayes class is _abstract_, that is, while it provides a blueprint for methods, the likelihood method is unspecified. That method encodes the particular likelihood function for, say, the Train problem, and therefore the Train class subclasses it and provides the working body of the likelihood method correct for the Train problem.\n",
    "\n",
    "Which is, by the way, that if the hypothesis says there are n trains, and the train number observed is m, then if m is greater than n, the hypothesis is discarded. It cannot be true. Its probability is set to zero. Otherwise, we give a likelihood of 1/n to the occurence, blindly assuming that of the n trains that we could have been sent our way, any of the n are equally likely to have been sent our way.\n",
    "\n",
    "Given the distribution of probabilities for each hypothesis, we next consider what our answer will be. The maximum likelihood approach seems inadequate. The solution by maximum likelihood is the maximum number of the observed trains.\n",
    "So we give two alternatives &mdash; the mean of the distribution and a credible interval, outside of which there is limited probability weight.\n",
    "\n",
    "\n",
    "<pre>\n",
    "       +--------+        references       +----------+\n",
    "       |  PMF   |---------------------+   |   Bayes  |\n",
    "       +--------+                     |   +----------+\n",
    "         |    | is-a +-------------+  |        | is-a\n",
    "         |    +------| Uniform PMF |  |   +-----------+\n",
    "         | is-a      +-------------+  +---|   Train   |\n",
    "         |      +---------------+         +-----------+\n",
    "         +------| Power Law PMF |\n",
    "                +---------------+\n",
    "</pre>\n",
    "\n",
    "\n",
    "__Sample of the solution__\n",
    "\n",
    "<pre>\n",
    "num= 1000 obs= [60]\n",
    "uniform prior 333.4198932637079\n",
    "credible interval (69, 869)\n",
    "power law prior 178.5473531797161\n",
    "credible interval (62, 559)\n",
    "\n",
    "num= 1000 obs= [30, 60, 90]\n",
    "uniform prior 164.30558642273346\n",
    "credible interval (92, 373)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "\n",
    "num= 500 obs= [30, 60, 90]\n",
    "uniform prior 151.84958795903836\n",
    "credible interval (92, 316)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "\n",
    "num= 2000 obs= [30, 60, 90]\n",
    "uniform prior 171.3381810915096\n",
    "credible interval (92, 393)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num= 1000 obs= [60]\n",
      "uniform prior 0.0\n",
      "credible interval (0, 0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e52347e42c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e52347e42c8c>\u001b[0m in \u001b[0;36mrun_bayes\u001b[0;34m(num, obs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uniform prior\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"credible interval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredible_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Pmf:\n",
    "    \n",
    "    def __init__(self,hypoth,prob):\n",
    "        \"\"\"\n",
    "        store a pair of vectors with the hypothesis and the probability fo that hypothesis\n",
    "        prob will be normalized, so it can be a relative frequency\n",
    "        hypoth and prob are lists, the initializer will convert to ndarrays\n",
    "        for the purposes of plotting hypoth are real numbers in increasing value\n",
    "        \"\"\"\n",
    "        assert len(hypoth)==len(prob)\n",
    "        self.n = 0    # remember the length, that's convenient\n",
    "        self.hypoth = None # turn hypoth into an ndarray\n",
    "        self.prob = None # turn prob into an ndarray\n",
    "        self.normalize()\n",
    "        \n",
    "    def normalize(self):\n",
    "        \"\"\"\n",
    "        normalize the prob vector\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def tell_distribution(self):\n",
    "        \"\"\"\n",
    "        return the pair (hypoth,prob), for e.g. plotting the distribution\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        return the mean of the distribution\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "    \n",
    "    def percentile(self,percent):\n",
    "        \"\"\"\n",
    "        return the hypoth such that the total probability\n",
    "        of being less that that hypoth is percent\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def credible_interval(self):\n",
    "        \"\"\"\n",
    "        return the pair (h1,h2) such at only 5% of hypothesis are\n",
    "        less than h1, and 5% of hypothesis are greater than h2\n",
    "        \"\"\"\n",
    "        return (0,0)\n",
    "        \n",
    "        \n",
    "class UniformDist(Pmf):\n",
    "    \"\"\"\n",
    "    a uniform distribution\n",
    "    \"\"\"\n",
    "    def __init__(self,n):\n",
    "        Pmf.__init__(self,range(1,n+1),[1.0]*n)\n",
    "        \n",
    "class PowerLawDist(Pmf):\n",
    "    \"\"\"\n",
    "    a power law distribution\n",
    "    \"\"\"\n",
    "    def __init__(self,n,alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        h = range(1,n+1)\n",
    "        p = [x**(-alpha) for x in h]\n",
    "        Pmf.__init__(self,h,p)\n",
    "\n",
    "class Bayes:\n",
    "    \n",
    "    def __init__(self,pmf):\n",
    "        assert isinstance(pmf,Pmf)\n",
    "        self.pmf = pmf\n",
    "    \n",
    "    def update(self,data):\n",
    "        \"\"\"\n",
    "        update the PMF given observation data\n",
    "        (1) get the likelihood of data given each hypoth\n",
    "        (2) multiple each prob by the likelihood\n",
    "        (3) normalize\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return self.pmf\n",
    "\n",
    "    def likelihood(self,data,hypo):\n",
    "        \"\"\"\n",
    "        this makes Bayes abstract. a concrete subclass implements this method\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class Train(Bayes):\n",
    "    \n",
    "    # inherit __init__\n",
    "    \n",
    "    def likelihood(self,data,hypo):\n",
    "        \"\"\"\n",
    "        the data is the train number; the hypo is the\n",
    "        hypothesis there are that many trains\n",
    "        (1) return 0.0 if data is larger than hypo, \n",
    "        because we learn that hypo is false;\n",
    "        (2) use the uniform prob over hypo number of\n",
    "        trains otherwise\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return 0.0\n",
    "\n",
    "def run_bayes(num,obs):\n",
    "    \n",
    "    print(\"num=\",num,\"obs=\",obs)\n",
    "    suite = Train(UniformDist(num))\n",
    "    for o in obs:\n",
    "        suite.update(o)\n",
    "    print(\"uniform prior\",suite.pmf.mean())\n",
    "    print(\"credible interval\",suite.pmf.credible_interval())\n",
    "    (h,p)= suite.pmf.tell_distribution()\n",
    "    plt.plot(h,p)\n",
    "\n",
    "    suite = Train(PowerLawDist(1000))\n",
    "    for o in obs:\n",
    "        suite.update(o)\n",
    "    print(\"power law prior\", suite.pmf.mean())\n",
    "    print(\"credible interval\",suite.pmf.credible_interval())\n",
    "    (h,p)= suite.pmf.tell_distribution()\n",
    "\n",
    "    plt.plot(h,p)\n",
    "    plt.xlabel(\"number of trains\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.legend([\"uniform\",\"power law\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "run_bayes(1000,[60])\n",
    "run_bayes(1000,[30,60,90])\n",
    "run_bayes(500,[30,60,90])\n",
    "run_bayes(2000,[30,60,90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
