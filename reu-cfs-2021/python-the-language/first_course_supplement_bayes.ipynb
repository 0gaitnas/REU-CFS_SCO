{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for REU 2019\n",
    "\n",
    "_Burt Rosenberg, 29 May 2019_\n",
    "\n",
    "\n",
    "__compared to as given in the Boot Camp, coalesed Bayes material from the lesson on Classes__\n",
    "\n",
    "\n",
    "### Supplement: Bayes\n",
    "\n",
    "This draws heavily from Thinking Bayes by Allen Downey.\n",
    "\n",
    "\n",
    "> In Baysian statistics, probably is analogous to belief. The higher the probablity the more one believes that the event is the case. It is the probability of the Weather Channel. Tomorrow will occur and it will occur only once &mdash; to say _\"50 out of 100 times it was March 22nd, 2019,  it rained\"_ makes no sense.\n",
    "\n",
    "The mantra of Bayesnian statistics is that the subjectivity is captured in the _prior_. But once that subjectivity is accounted for, the rest is objective. Data will update that prior by providing supporting or refuting evidence, and the posterior probability reflects the rational person's belief, given the prior and the evidence.\n",
    "\n",
    "The formula is, given <code>P(H<sub>i</sub>)</code>, the prior of the probability of <code>H<sub>i</sub></code>, and the occurence <code>D</code>, what is the new belief that <code>H<sub>i</sub></code> is the case? I.e. <code>P(H<sub>i</sub>|D)</code>? This is derived as \n",
    "\n",
    "> <code>P(H<sub>i</sub>|D) P(D) = P(D|H<sub>i</sub>) P(H<sub>i</sub>)</code>\n",
    "\n",
    "the symbol <code>P(D|H<sub>i</sub>)</code> is called the _likelihood_. One avoids calculating the <code>P(D)</code> normalizing factor by normalizing instead normalizing the collection <code>{P(H<sub>i</sub>|D) P(H<sub>i</sub>)}<sub>i</sub></code>, given that the <code>H<sub>i</sub></code> are mutually exclusive and collectively exhaustive.\n",
    "\n",
    "\n",
    "__The M&amp;M Problem__\n",
    "\n",
    "Solve the M&M Problem as given in Allen Downey, Think Bayes. See http://www.greenteapress.com/thinkbayes/html/thinkbayes002.html#sec14\n",
    "\n",
    "In Think Bayes, Prof. Downey gives his own set of classes to calculate solutions in Bayesian statistics. The M&M problem is the problem of deciding the manufacturing year of a bag of M&M's: given two bags of M&M, one from 1994 and one from 1996, but it is unknown which is the '94 and which is the '96, find the likeliest given that a random drawing of one M&M from each bag gave a yellow and a green.\n",
    "\n",
    "Now, first off, why do we have M&M's from 1994? \n",
    "\n",
    "Second, because blue M&M's were introduced in 1995, the color mix inside the bags changed. In extermis, if a blue was drawn from one of the bags, when we know exactly which bag is not from 1994!\n",
    "\n",
    "This is a case of Bayesian statistics: we have a prior guess &mdash; since the bags came to us randomly it is 50/50 the first is from 1994 and the second from 1996. We get some data: the first bag drew a yellow, the second bag drew and green. Then we update the prior belief to a posterior belief by incorporating this data.\n",
    "\n",
    "In the language of conditional probability, let H be the case that the first bag is from '94 and the second from 96', and D be the event of a yellow drawn from the first bag and a green from the second. We know P(H) how do we update that to P(H|D)?\n",
    "\n",
    "Bayes Law is crucial (and from where Bayesian Statistics gets the name). It says:\n",
    "\n",
    "> P(H|D) = P(D|H) P(H) / P(D)\n",
    "\n",
    "P(H) is called the prior; P(H|D) the posterior; and P(D|H) the likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken!\n",
      "broken!\n"
     ]
    }
   ],
   "source": [
    "# Fix my broken code\n",
    "\n",
    "\n",
    "class Likelihood:\n",
    "    \n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "\n",
    "    def set_prior(self,d):\n",
    "        if len(d)!=self.n:\n",
    "            # this shows how to raise in exception\n",
    "            raise Exception(\"incorrect dimension\")\n",
    "        # the slice notation d[:] causes a copy, see next lesson\n",
    "        self.d = d[:]\n",
    "        \n",
    "    def set_likelihood(self,lh):\n",
    "        if len(lh)!=self.n:\n",
    "            raise Exception(\"incorrect dimension\")\n",
    "        self.lh = lh[:]\n",
    "        \n",
    "    def update_post(self):\n",
    "        # update the list d to the posterior values\n",
    "        # do this by producting d[i], the prior, with lh[i], the likelihood\n",
    "        # then normalizing the result (sum up and divide each by the sum)\n",
    "        #\n",
    "        pass\n",
    "        \n",
    "    def tell_dist(self):\n",
    "        return self.d\n",
    "    \n",
    "    def tell_likeliest(self):\n",
    "        # return the index i such that self.d[i] is maxium\n",
    "        return -1\n",
    "\n",
    "            \n",
    "def m_and_m_problem():\n",
    "    \"\"\"\n",
    "    From Allen Downey, Think Bayes\n",
    "    1994 M&M bag: 30% brown, 20% yellow, 20% red, 10% green, 10% orange, 10% tan\n",
    "    1996 M&M bag: 24% blue, 20% green, 16% orange, 14% yellow, 13% red, 13% brown\n",
    "    \n",
    "    a yellow is drawn from one bag, and a green from the other\n",
    "    \n",
    "    what is the likelihood tye yellow came from a 1994 bag?\n",
    "    \"\"\"\n",
    "    \n",
    "    n = 2  # there are two cases, 0: yellow from 1994, 1: yellow from 1996\n",
    "    prior = [0.5,0.5]  # we have no idea which case we are in\n",
    "    likelihood = [\n",
    "        .2*.2, # if in case 0, independent prob yellow from 1994 and green from 1996\n",
    "        .1*.14 # if in case 1, independent probl green from 1994 and yellow from 1996\n",
    "    ]\n",
    "    \n",
    "    l_h = Likelihood(n)\n",
    "    l_h.set_prior(prior)\n",
    "    l_h.set_likelihood(likelihood)\n",
    "    l_h.update_post()\n",
    "    \n",
    "    # note: using a tuple to return multiple values. tuples are immutable\n",
    "    return (l_h.tell_dist(),l_h.tell_likeliest())\n",
    "\n",
    "def test_m_and_m_problem():\n",
    "    # the answers\n",
    "    ans = [0.74, 0.25]\n",
    "    ans_l = 0\n",
    "    \n",
    "    # tuple assignment\n",
    "    (res,likeliest) = m_and_m_problem() \n",
    "    \n",
    "    # shows how to loop over the indices (range(len())), \n",
    "    # and how to do list comprehension [ x for la la la ]\n",
    "    # and how to use max over a list\n",
    "    err = max([(ans[i]-res[i])**2 for i in range(len(ans))])\n",
    "    \n",
    "    if likeliest!=ans_l:\n",
    "        print(\"broken!\")\n",
    "    if err<0.01:\n",
    "        print(\"correct!\")\n",
    "    else:\n",
    "        print(\"broken!\")\n",
    "        \n",
    "test_m_and_m_problem()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__The Train Problem__\n",
    "\n",
    "We want to estimate the number of trains owned by a certain railroad company, given the observation of trains, and the train number. We know that the company numbers its trains consecutively starting with one. If we observe train 60, for instance, then we know that company owns at least 60 trains. We make several measurements and then provide a guess as to how many trains we haven't seen.\n",
    "\n",
    "This problem considers two priors. One prior is a uniform distribution over a range from 1 to n. That is, we assume there are some trains, but never more than n, and we have no preference for one number of trains compared to another. This results in some answer. However, things being what they are, it seems silly to think that there is a magic n, and all train companies choose a number uniformly between 1 and n, and that's the number of trains they run. Rather we suspect that the number of trains a company owns follows a power law. And using a power law distribution we find that our estimations are less sensitive to the arbitrary parameter choices we make when choosing a prior.\n",
    "\n",
    "That's a good thing.\n",
    "\n",
    "The code demonstrates some features of classes. There is a base class for a Probability Mass Function, and particular PMF's inherit from it. They enhance the class by initilizing a discretized PMF according to either the Uniform or the Power Law distribution.\n",
    "\n",
    "The Bayes class contains the basic framework for estimation. On creation it is provided a prior, in the form of an initialized PMF object. The exact type of the PMF object will be a subclass, depending on distribution, but all share the class PMF which, and this is the functionality needed for the Bayes class. \n",
    "\n",
    "The Bayes class is _abstract_, that is, while it provides a blueprint for methods, the likelihood method is unspecified. That method encodes the particular likelihood function for, say, the Train problem, and therefore the Train class subclasses it and provides the working body of the likelihood method correct for the Train problem.\n",
    "\n",
    "Which is, by the way, that if the hypothesis says there are n trains, and the train number observed is m, then if m is greater than n, the hypothesis is discarded. It cannot be true. Its probability is set to zero. Otherwise, we give a likelihood of 1/n to the occurence, blindly assuming that of the n trains that we could have been sent our way, any of the n are equally likely to have been sent our way.\n",
    "\n",
    "Given the distribution of probabilities for each hypothesis, we next consider what our answer will be. The maximum likelihood approach seems inadequate. The solution by maximum likelihood is the maximum number of the observed trains.\n",
    "So we give two alternatives &mdash; the mean of the distribution and a credible interval, outside of which there is limited probability weight.\n",
    "\n",
    "\n",
    "<pre>\n",
    "       +--------+        references       +----------+\n",
    "       |  PMF   |---------------------+   |   Bayes  |\n",
    "       +--------+                     |   +----------+\n",
    "         |    | is-a +-------------+  |        | is-a\n",
    "         |    +------| Uniform PMF |  |   +-----------+\n",
    "         | is-a      +-------------+  +---|   Train   |\n",
    "         |      +---------------+         +-----------+\n",
    "         +------| Power Law PMF |\n",
    "                +---------------+\n",
    "</pre>\n",
    "\n",
    "\n",
    "__Sample of the solution__\n",
    "\n",
    "<pre>\n",
    "num= 1000 obs= [60]\n",
    "uniform prior 333.4198932637079\n",
    "credible interval (69, 869)\n",
    "power law prior 178.5473531797161\n",
    "credible interval (62, 559)\n",
    "\n",
    "num= 1000 obs= [30, 60, 90]\n",
    "uniform prior 164.30558642273346\n",
    "credible interval (92, 373)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "\n",
    "num= 500 obs= [30, 60, 90]\n",
    "uniform prior 151.84958795903836\n",
    "credible interval (92, 316)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "\n",
    "num= 2000 obs= [30, 60, 90]\n",
    "uniform prior 171.3381810915096\n",
    "credible interval (92, 393)\n",
    "power law prior 133.27523137503107\n",
    "credible interval (91, 242)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num= 1000 obs= [60]\n",
      "uniform prior 0.0\n",
      "credible interval (0, 0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e52347e42c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mrun_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e52347e42c8c>\u001b[0m in \u001b[0;36mrun_bayes\u001b[0;34m(num, obs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uniform prior\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"credible interval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredible_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Pmf:\n",
    "    \n",
    "    def __init__(self,hypoth,prob):\n",
    "        \"\"\"\n",
    "        store a pair of vectors with the hypothesis and the probability fo that hypothesis\n",
    "        prob will be normalized, so it can be a relative frequency\n",
    "        hypoth and prob are lists, the initializer will convert to ndarrays\n",
    "        for the purposes of plotting hypoth are real numbers in increasing value\n",
    "        \"\"\"\n",
    "        assert len(hypoth)==len(prob)\n",
    "        self.n = 0    # remember the length, that's convenient\n",
    "        self.hypoth = None # turn hypoth into an ndarray\n",
    "        self.prob = None # turn prob into an ndarray\n",
    "        self.normalize()\n",
    "        \n",
    "    def normalize(self):\n",
    "        \"\"\"\n",
    "        normalize the prob vector\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def tell_distribution(self):\n",
    "        \"\"\"\n",
    "        return the pair (hypoth,prob), for e.g. plotting the distribution\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        return the mean of the distribution\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "    \n",
    "    def percentile(self,percent):\n",
    "        \"\"\"\n",
    "        return the hypoth such that the total probability\n",
    "        of being less that that hypoth is percent\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def credible_interval(self):\n",
    "        \"\"\"\n",
    "        return the pair (h1,h2) such at only 5% of hypothesis are\n",
    "        less than h1, and 5% of hypothesis are greater than h2\n",
    "        \"\"\"\n",
    "        return (0,0)\n",
    "        \n",
    "        \n",
    "class UniformDist(Pmf):\n",
    "    \"\"\"\n",
    "    a uniform distribution\n",
    "    \"\"\"\n",
    "    def __init__(self,n):\n",
    "        Pmf.__init__(self,range(1,n+1),[1.0]*n)\n",
    "        \n",
    "class PowerLawDist(Pmf):\n",
    "    \"\"\"\n",
    "    a power law distribution\n",
    "    \"\"\"\n",
    "    def __init__(self,n,alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        h = range(1,n+1)\n",
    "        p = [x**(-alpha) for x in h]\n",
    "        Pmf.__init__(self,h,p)\n",
    "\n",
    "class Bayes:\n",
    "    \n",
    "    def __init__(self,pmf):\n",
    "        assert isinstance(pmf,Pmf)\n",
    "        self.pmf = pmf\n",
    "    \n",
    "    def update(self,data):\n",
    "        \"\"\"\n",
    "        update the PMF given observation data\n",
    "        (1) get the likelihood of data given each hypoth\n",
    "        (2) multiple each prob by the likelihood\n",
    "        (3) normalize\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return self.pmf\n",
    "\n",
    "    def likelihood(self,data,hypo):\n",
    "        \"\"\"\n",
    "        this makes Bayes abstract. a concrete subclass implements this method\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class Train(Bayes):\n",
    "    \n",
    "    # inherit __init__\n",
    "    \n",
    "    def likelihood(self,data,hypo):\n",
    "        \"\"\"\n",
    "        the data is the train number; the hypo is the\n",
    "        hypothesis there are that many trains\n",
    "        (1) return 0.0 if data is larger than hypo, \n",
    "        because we learn that hypo is false;\n",
    "        (2) use the uniform prob over hypo number of\n",
    "        trains otherwise\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return 0.0\n",
    "\n",
    "def run_bayes(num,obs):\n",
    "    \n",
    "    print(\"num=\",num,\"obs=\",obs)\n",
    "    suite = Train(UniformDist(num))\n",
    "    for o in obs:\n",
    "        suite.update(o)\n",
    "    print(\"uniform prior\",suite.pmf.mean())\n",
    "    print(\"credible interval\",suite.pmf.credible_interval())\n",
    "    (h,p)= suite.pmf.tell_distribution()\n",
    "    plt.plot(h,p)\n",
    "\n",
    "    suite = Train(PowerLawDist(1000))\n",
    "    for o in obs:\n",
    "        suite.update(o)\n",
    "    print(\"power law prior\", suite.pmf.mean())\n",
    "    print(\"credible interval\",suite.pmf.credible_interval())\n",
    "    (h,p)= suite.pmf.tell_distribution()\n",
    "\n",
    "    plt.plot(h,p)\n",
    "    plt.xlabel(\"number of trains\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.legend([\"uniform\",\"power law\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "run_bayes(1000,[60])\n",
    "run_bayes(1000,[30,60,90])\n",
    "run_bayes(500,[30,60,90])\n",
    "run_bayes(2000,[30,60,90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
